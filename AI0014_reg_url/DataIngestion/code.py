#!/usr/bin/env python
# -*- coding: utf-8 -*-
'''
This file is automatically generated by AION for AI0014_1 usecase.
File generation time: 2023-02-20 16:38:27
'''
#Standard Library modules
import json
import argparse
import platform

#Third Party modules
from pathlib import Path
import pandas as pd 

#local modules
from utility import *
from data_reader import dataReader

IOFiles = {
    "rawData": "rawData.dat",
    "metaData": "modelMetaData.json",
    "log": "aion.log",
    "outputData": "rawData.dat",
    "monitoring": "monitoring.json",
    "prodData": "prodData",
    "prodDataGT": "prodDataGT"
}
        
def validateConfig():        
    config_file = Path(__file__).parent/'config.json'        
    if not Path(config_file).exists():        
        raise ValueError(f'Config file is missing: {config_file}')        
    config = read_json(config_file)		
    if not config['targetPath']:        
        raise ValueError(f'Target Path is not configured')        
    return config

#This function will read the data and save the data on persistent storage        
def load_data(log):        
    config = validateConfig()
    targetPath = Path('aion')/config['targetPath']       
    targetPath.mkdir(parents=True, exist_ok=True)	
    log_file = targetPath/IOFiles['log']        
    log = logger(log_file, mode='a', logger_name=Path(__file__).parent.stem)
    monitoring = targetPath/IOFiles['monitoring']	
    if monitoring.exists():        
        monitoringStatus = read_json(monitoring)
        if monitoringStatus['dataLocation'] == '' and monitoringStatus['driftStatus'] != 'No Drift':
            reader = dataReader(reader_type=monitoring_data.get('prod_db_type','sqlite'),target_path=targetPath, config=config.get('db_config',None))
            
            raw_data_location = targetPath/IOFiles['rawData']		
            if reader.file_exists(IOFiles['prodData']) and reader.file_exists(IOFiles['prodDataGT']): 		
                predicted_data = reader.read(IOFiles['prodData'])
                actual_data = reader.read(IOFiles['prodDataGT'])
                common_col = [k for k in predicted_data.columns.tolist() if k in actual_data.columns.tolist()]				
                mergedRes = pd.merge(actual_data, predicted_data, on =common_col,how = 'inner')
                raw_data_path = pd.read_csv(raw_data_location)			
                df = pd.concat([raw_data_path,mergedRes])
            else:
                raise ValueError(f'Prod Data not found')			
        elif monitoringStatus['dataLocation'] == '':
            raise ValueError(f'Data Location does not exist')
        else:
            if 's3' in monitoringStatus.keys():
                input_reader = dataReader(reader_type='s3',target_path=None, config=monitoringStatus['s3'])
                log.info(f"Downloading '{monitoringStatus['s3']['file_name']}' from s3 bucket '{monitoringStatus['s3']['bucket_name']}'")
                df = input_reader.read(monitoringStatus['s3']['file_name'])
            else:
                location = monitoringStatus['dataLocation']
                log.info(f'Dataset path: {location}')
                df = read_data(location)
    else:
        raise ValueError(f'Monitoring.json does not exist')		

    status = {}        
    output_data_path = targetPath/IOFiles['outputData']       
    log.log_dataframe(df)        
    required_features = list(set(config['selected_features'] + [config['target_feature']]))        
    log.info('Dataset features required: ' + ','.join(required_features))        
    missing_features = [x for x in required_features if x not in df.columns.tolist()]        
    if missing_features:        
        raise ValueError(f'Some feature/s is/are missing: {missing_features}')        
    log.info('Removing unused features: '+','.join(list(set(df.columns) - set(required_features))))        
    df = df[required_features]        
    log.info(f'Required features: {required_features}')        
    try:        
        log.info(f'Saving Dataset: {str(output_data_path)}')        
        write_data(df, output_data_path, index=False)        
        status = {'Status':'Success','DataFilePath':IOFiles['outputData'],'Records':len(df)}        
    except:        
        raise ValueError('Unable to create data file')        
        
    meta_data_file = targetPath/IOFiles['metaData']        
    meta_data = dict()        
    meta_data['load_data'] = {}        
    meta_data['load_data']['selected_features'] = [x for x in config['selected_features'] if x != config['target_feature']]        
    meta_data['load_data']['Status'] = status        
    write_json(meta_data, meta_data_file)        
    output = json.dumps(status)        
    log.info(output)        
    return output

        
if __name__ == '__main__':        
    log = None        
    try:        
        print(load_data(log))        
    except Exception as e:        
        if log:        
            log.getLogger().error(e, exc_info=True)        
        status = {'Status':'Failure','Message':str(e)}        
        print(json.dumps(status))        
        raise Exception(str(e))        