#!/usr/bin/env python
# -*- coding: utf-8 -*-
'''
This file is automatically generated by AION for AI0014_1 usecase.
File generation time: 2023-02-20 16:38:29
'''
#Standard Library modules
import sys
import json
import math
import argparse
import platform

#Third Party modules
import joblib
import mlflow
import sklearn
from pathlib import Path
import numpy as np 
import pandas as pd 
from scipy import stats as st 

#local modules
from utility import *
from data_reader import dataReader
from input_drift import inputdrift

IOFiles = {
    "log": "aion.log",
    "trainingData": "rawData.dat",
    "production": "production.json",
    "monitoring": "monitoring.json",
    "prodData": "prodData",
    "prodDataGT": "prodDataGT"
}	
def is_drift_within_limits(production, current_matrices,scoring_criteria,threshold = 5):
    testscore = production['score']
    current_score = current_matrices[scoring_criteria]
    threshold_value = testscore * threshold / 100.0

    if current_score > (testscore - threshold_value) :
        return True
    else:
        return False

def get_metrices(actual_values, predicted_values):        
    import numpy as np
    result = {}
    me = np.mean(predicted_values - actual_values)
    sde = np.std(predicted_values - actual_values, ddof = 1)

    abs_err = np.abs(predicted_values - actual_values)
    mae = np.mean(abs_err)
    sdae = np.std(abs_err, ddof = 1)

    abs_perc_err = 100.0 * np.abs(predicted_values - actual_values) / actual_values
    mape = np.mean(abs_perc_err)
    sdape = np.std(abs_perc_err, ddof = 1)

    result['mean_error'] = me
    result['mean_abs_error'] = mae
    result['mean_abs_perc_error'] = mape
    result['error_std'] = sde
    result['abs_error_std'] = sdae
    result['abs_perc_error_std'] = sdape
    return result
            	                
def monitoring(config, log=None):
    targetPath = Path('aion')/config['targetPath']
    targetPath.mkdir(parents=True, exist_ok=True)	 	
    log_file = targetPath/IOFiles['log']
    log = logger(log_file, mode='a', logger_name=Path(__file__).parent.stem)
    output_json = {}
    trainingDataLocation = targetPath/IOFiles['trainingData']
    monitoring = targetPath/IOFiles['monitoring']	
    log.info(f'Input Location External: {config["inputUriExternal"]}')
    trainingStatus = 'False'
    dataFileLocation = ''	
    driftStatus = 'No Drift'
    if monitoring.exists(): 	
        monitoring_data = read_json(monitoring)
        if monitoring_data.get('runNo', False):
            reader = dataReader(reader_type=monitoring_data.get('prod_db_type','sqlite'),target_path=targetPath, config=config.get('db_config',None))
            production= targetPath/IOFiles['production']
            proddataDF = pd.DataFrame()
            predicted_data = pd.DataFrame() 		
            if production.exists():        
                production = read_json(production)  
                if reader.file_exists(IOFiles['prodData']) and reader.file_exists(IOFiles['prodDataGT']):
                    predicted_data = reader.read(IOFiles['prodData'])       		
                    actual_data = reader.read(IOFiles['prodDataGT'])
                    common_col = [k for k in predicted_data.columns.tolist() if k in actual_data.columns.tolist()]				
                    proddataDF = pd.merge(actual_data, predicted_data, on =common_col,how = 'inner')
                    currentPerformance = {} 			
                    currentPerformance = get_metrices(proddataDF[config['target_feature']], proddataDF['prediction'])
                    if is_drift_within_limits(production, currentPerformance,config['scoring_criteria']):
                        log.info(f'OutputDrift: No output drift found')				
                        output_json.update({'outputDrift':'Model score is with in limits'}) 					
                    else:
                        log.info(f'OutputDrift: Found Output Drift')				
                        log.info(f'Original Test Score: {production["score"]}')					
                        log.info(f'Current Score: {currentPerformance[config["scoring_criteria"]]}')				
                        output_json.update({'outputDrift':{'Meassage': 'Model output is drifted','trainedScore':production["score"], 'currentScore':currentPerformance[config["scoring_criteria"]]}})
                        trainingStatus = 'True'
                        driftStatus = 'Output Drift'					
                else:
                    if reader.file_exists(IOFiles['prodData']):
                        predicted_data = reader.read(IOFiles['prodData'])            
                    log.info(f'OutputDrift: Prod Data not found')			
                    output_json.update({'outputDrift':'Prod Data not found'}) 
            else:
                log.info(f'Last Time pipeline not executed completely')		
                output_json.update({'Msg':'Pipeline is not executed completely'})
                trainingStatus = 'True'
                if config['inputUriExternal']:		
                    dataFileLocation = config['inputUriExternal']
                elif 's3' in config.keys():
                    dataFileLocation = 'cloud'
                else:
                    dataFileLocation = config['inputUri']			
                    
                
            if trainingStatus == 'False':	       			
                historicaldataFrame=pd.read_csv(trainingDataLocation)
                if config['inputUriExternal']:			
                    currentdataFrame=pd.read_csv(config['inputUriExternal'])
                elif not predicted_data.empty:
                    currentdataFrame = predicted_data.copy()            			
                elif 's3' in config.keys():
                    reader = dataReader(reader_type='s3',target_path=config['targetPath'], config=config['s3'])
                    currentdataFrame = reader.read(config['s3']['file_name'])
                else:
                    currentdataFrame=pd.read_csv(config['inputUri']) 			
                inputdriftObj = inputdrift(config)
                dataalertcount,inputdrift_message = inputdriftObj.get_input_drift(currentdataFrame,historicaldataFrame)	

                if inputdrift_message == 'Model is working as expected':
                    log.info(f'InputDrift: No input drift found')			
                    output_json.update({'Status':'SUCCESS','inputDrift':'Model is working as expected'})        
                else:        
                    log.info(f'InputDrift: Input drift found')			
                    log.info(f'Affected Columns {inputdrift_message}')			
                    output_json.update({'inputDrift':{'Affected Columns':inputdrift_message}})        
                    trainingStatus = 'True'
                    driftStatus = 'Input Drift'                
                    if config['inputUriExternal']:
                        dataFileLocation = config['inputUriExternal']
                    elif actual_data_path.exists() and predict_data_path.exists():	
                        dataFileLocation = ''
                    elif 's3' in config.keys():
                        dataFileLocation = 'cloud'
                    else:
                        dataFileLocation = config['inputUri']					
        else:
            log.info(f'Pipeline Executing first Time')	
            output_json.update({'Msg':'Pipeline executing first time'}) 	
            trainingStatus = 'True'
            if config['inputUriExternal']:		
                dataFileLocation = config['inputUriExternal']
            elif 's3' in config.keys():
                dataFileLocation = 'cloud'
            else:
                dataFileLocation = config['inputUri']
    else:
        log.info(f'Pipeline Executing first Time')	
        output_json.update({'Msg':'Pipeline executing first time'}) 	
        trainingStatus = 'True'
        if config['inputUriExternal']:		
            dataFileLocation = config['inputUriExternal']
        elif 's3' in config.keys():
            dataFileLocation = 'cloud'
        else:
            dataFileLocation = config['inputUri']
    if monitoring.exists():
        monitoring_data['runNo'] = int(monitoring_data.get('runNo', '0')) + 1		
    else:
        monitoring_data = {}
        monitoring_data['runNo'] = 1
        monitoring_data['prod_db_type'] = config.get('prod_db_type', 'sqlite')
        monitoring_data['db_config'] = config.get('db_config', {})
        monitoring_data['mlflow_config'] = config.get('mlflow_config', None)
        if 's3' in config.keys():
            monitoring_data['s3'] = config['s3']
    monitoring_data['dataLocation'] = dataFileLocation
    monitoring_data['driftStatus'] = driftStatus
    write_json(monitoring_data,targetPath/IOFiles['monitoring'])
    output = {'Status':'SUCCESS'}
    output.update(output_json)
    return(json.dumps(output))       
        	
if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('-i', '--inputUri', help='Training Data Location')

    args = parser.parse_args()
    config_file = Path(__file__).parent/'config.json'
    if not Path(config_file).exists():
        raise ValueError(f'Config file is missing: {config_file}')
    config = read_json(config_file)
    config['inputUriExternal'] = None
    if args.inputUri:
        if 	args.inputUri != '':
            config['inputUriExternal'] = args.inputUri
    log = None
    try:
        print(monitoring(config, log))
    except Exception as e:
        if log:
            log.error(e, exc_info=True)
        status = {'Status':'Failure','Message':str(e)}
        print(json.dumps(status))
        raise Exception(str(e))
